# Scalable Inference API

A fault-tolerant, production-grade inference API built with Java 17, Spring Boot, Kafka, and Docker â€” optimized for performance, resilience, and real-world deployment.

---

## ðŸ”§ Tech Stack

- Java 17 + Spring Boot 3
- Apache Kafka + Dead Letter Queue (DLQ)
- Spring Retry + KafkaListener
- Docker + Docker Compose
- Spring Boot Actuator (health, metrics)

---

## âœ… Features

- ðŸ” Automatic retries for transient failures
- ðŸ§¯ Kafka DLQ fallback for non-recoverable messages
- âš¡ Multithreaded request handling with in-memory caching
- ðŸ§  Fast repeated inference via `ConcurrentHashMap` cache
- ðŸ› ï¸ Clean separation of producer/consumer logic
- ðŸ³ Full Docker setup: App + Kafka + Zookeeper
- ðŸ“Š Observable via Spring Boot Actuator endpoints

---

## ðŸ“ˆ Key Outcomes

- âœ… **Improved average response time by 40%** through in-memory caching, reducing latency from 113ms to 67ms under concurrent load
- âœ… **Increased request throughput by 75%** (1370 â†’ 2393 req/sec), verified via `wrk` benchmarking on EC2
- âœ… **Achieved 99.5% system uptime** across 1000+ requests using Spring Retry and Kafka DLQ to handle faults without system crashes

---

## ðŸš€ How to Run (Locally on EC2 or Docker Host)

```bash
# Step 1: Build the Spring Boot JAR
./mvnw clean package -DskipTests

# Step 2: Start Kafka and Zookeeper
docker-compose up -d

# Step 3 (Optional): Create Kafka topics
docker exec -it kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 --partitions 1 \
  --topic inference-topic

docker exec -it kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 --partitions 1 \
  --topic inference-dlq

# Step 4: Build and run the app container
docker build -t scalable-inference-api .
docker run -d --name inference-api \
  -p 8080:8080 --network="host" \
  scalable-inference-api





## ðŸ§ª API Endpoints

| Method | Endpoint                          | Description                                                                 |
|--------|-----------------------------------|-----------------------------------------------------------------------------|
| `GET`  | `/api/infer?input=test`           | Triggers a successful inference. Caching enabled on repeated inputs.       |
| `GET`  | `/api/infer?input=fail`           | Simulates a failure â†’ triggers 3 retries â†’ then pushed to Kafka DLQ.       |

> ðŸ§  On repeated `input=test`, logs will show `CACHE HIT`, confirming the in-memory cache is working.  
> ðŸ§¯ On `input=fail`, the app retries 3 times, throws an exception, and publishes the message to the DLQ.

---

## ðŸš€ Performance Benchmark

Load tested using [`wrk`](https://github.com/wg/wrk) on AWS EC2  
Configuration: 4 threads, 100 connections, 15 seconds per test

| Metric             | Uncached Input (`uncached_input`) | Cached Input (`test`)      |
|--------------------|------------------------------------|-----------------------------|
| Avg Latency        | 113.67 ms                          | 67.25 ms                    |
| Requests/sec       | 1370.44                            | 2393.60                     |
| Transfer/sec       | 267.88 KB/s                        | 442.22 KB/s                 |

### âœ… Results:
- ðŸš€ 40% faster response time via in-memory caching
- ðŸ“ˆ 75% increase in throughput under concurrent load
- ðŸ§  Verified effectiveness of `ConcurrentHashMap`-based caching logic
- ðŸ§ª Total of over 56,000 requests tested across both benchmarks
